---
layout: post
title: "A 'wedding cake' probability distribution -- Part 2"
author: "Chris Langfield"
categories: math
tags: [math]
---

In [part 1](https://chris-langfield.github.io/wedding-cake) we explored the discrete probability distribution generated by a process of "double selection" from a set of integers. First we pick $$X_0 \sim unif(1\dots k)$$, and then we pick $$X_1 \sim unif(1\dots X_0)$$. The probability distribution of $$X_1$$ is

$$ Pr(X_1 = s) = \frac{1}{k} (H_k - H_{s-1}) $$

We proposed a sequence of probability distributions -- let's now call them $$p^k_m$$ -- corresponding to repeating this process $$m$$ times. That is, we pick an integer $$X_0$$ uniformly from $$1 \dots k$$, then we pick an integer $$X_1$$ uniformly from $$1 \dots X_0$$, pick $$X_2$$ from $$1 \dots X_1$$ and so on. Define 

$$p^k_0(s) = \frac{1}{k}$$

i.e. the base case of this process is simply the discrete uniform distribution. Then,

$$p^k_1(s) = \frac{1}{k} (H_k - H_{s-1})$$

This is what was derived in the last post. I mentioned that it is quite difficult to continue finding general expressions for $$p^k_m(s)$$ directly. However, there is a pattern with these distributions. First, recall:

$$Pr(X_m = s) = \sum_{i=1}^k Pr(X_m = s | X_{m-1} = i) Pr(X_{m-1} = i) $$

Using the same reasoning as in Part 1, we argue that 

$$
Pr(X_m=s|X_{m-1}=i) = \begin{cases} 
    \frac{1}{i} & s\leq i \\
    0 & s > i \\
  \end{cases}
$$

for every $$m$$. This is due to the fact that once $$X_{m-1}$$ has been chosen, $$X_m$$ is sampled *uniformly* from $$1\dots X_{m-1}$$

Therefore,

$$p^k_m(s) = \sum_{i=s}^k \frac{1}{i} p^k_{m-1}(i)$$

This recursive relationship can be expanded into $$k$$ equations:
$$
    p_m(1) = 1 \cdot p_{m-1}(1) + \bigg(\frac{1}{2}\bigg)p_{m-1}(2) + \bigg(\frac{1}{3}\bigg)p_{m-1}(3) + \dots + \bigg(\frac{1}{k-1}\bigg)p_{m-1}(k-1) + \bigg(\frac{1}{k}\bigg)p_{m-1}(k) \\
    p_m(2) = 0  + \bigg(\frac{1}{2}\bigg) p_{m-1}(2) + \bigg(\frac{1}{3}\bigg)p_{m-1}(3) + \dots + \bigg(\frac{1}{k-1}\bigg)p_{m-1}(k-1) + \bigg(\frac{1}{k}\bigg)p_{m-1}(k) \\
    p_m(3) = 0  + 0 + \bigg(\frac{1}{3}\bigg)p_{m-1}(3) + \dots + \bigg(\frac{1}{k-1}\bigg)p_{m-1}(k-1) + \bigg(\frac{1}{k}\bigg)p_{m-1}(k) \\
    p_m(4) = 0 + 0 + 0  + \dots + \bigg(\frac{1}{k-1}\bigg)p_{m-1}(k-1) + \bigg(\frac{1}{k}\bigg)p_{m-1}(k) \\
    \dots \\
    p_m(k) = 0 + 0 + 0  + \dots + 0 + \bigg(\frac{1}{k}\bigg)p_{m-1}(k) \\
$$

If we now consider $$p^k_m$$ as a probability vector on the set $$S = \{1\dots k\}$$, then we have a matrix equation:

$$
    \begin{pmatrix}
      p_m(1) \\
      p_m(2) \\
      \dots \\
      p_m(k)
    \end{pmatrix}
    =
    \begin{pmatrix}
      1 & \frac{1}{2} & \frac{1}{3} & \dots & \frac{1}{k-1} & \frac{1}{k} \\
      0 & \frac{1}{2} & \frac{1}{3} & \dots  & \frac{1}{k-1} & \frac{1}{k} \\
      0 & 0 & \frac{1}{3} & \dots & \frac{1}{k-1} & \frac{1}{k} \\
      \vdots & \vdots & \vdots & \ddots & \frac{1}{k-1} & \frac{1}{k} \\
      0 & 0 & 0 & 0 & 0 & \frac{1}{k} 
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
      p_{m-1}(1) \\
      p_{m-1}(2) \\
      \dots \\
      p_{m-1}(k)\\
    \end{pmatrix}
$$

