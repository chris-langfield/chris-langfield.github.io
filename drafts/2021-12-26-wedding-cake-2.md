---
layout: post
title: "A 'wedding cake' probability distribution -- Part 2"
author: "Chris Langfield"
categories: math
tags: [math]
---

**You found the secret drafts area! Keep in mind the material below is in progress and subject to change prior to publishing.**

In [part 1](https://chris-langfield.github.io/wedding-cake) we explored the discrete probability distribution generated by a process of "double selection" from a set of integers. First we pick $$X_0 \sim \text{unif}(1\dots k)$$, and then we pick $$X_1 \sim \text{unif}(1\dots X_0)$$. The probability distribution of $$X_1$$ is

$$ Pr(X_1 = s) = \frac{1}{k} (H_k - H_{s-1}) $$

We proposed a sequence of probability distributions -- let's now call them $$p^k_m$$ -- corresponding to repeating this process $$m$$ times. That is, we pick an integer $$X_0$$ uniformly from $$1 \dots k$$, then we pick an integer $$X_1$$ uniformly from $$1 \dots X_0$$, pick $$X_2$$ from $$1 \dots X_1$$ and so on. Define 

$$p^k_0(s) = \frac{1}{k}$$

i.e. the base case of this process is simply the discrete uniform distribution. Then,

$$p^k_1(s) = \frac{1}{k} (H_k - H_{s-1})$$

This is what was derived in the last post. I mentioned that it is quite difficult to continue finding general expressions for $$p^k_m(s)$$ directly. However, there is a pattern with these distributions. First, recall:

$$Pr(X_m = s) = \sum_{i=1}^k Pr(X_m = s | X_{m-1} = i) Pr(X_{m-1} = i) $$

Using the same reasoning as in Part 1, we argue that 

$$
Pr(X_m=s|X_{m-1}=i) = \begin{cases} 
    \frac{1}{i} & s\leq i \\
    0 & s > i \\
  \end{cases}
$$

for every $$m$$. This is due to the fact that once $$X_{m-1}$$ has been chosen, $$X_m$$ is sampled *uniformly* from $$1\dots X_{m-1}$$

Therefore,

$$p^k_m(s) = \sum_{i=s}^k \frac{1}{i} p^k_{m-1}(i)$$

This recursive relationship can be expanded into $$k$$ equations:
$$
p^k_m(1) = 1 \cdot p_{m-1}(1) + \bigg(\frac{1}{2}\bigg)p^k_{m-1}(2) + \bigg(\frac{1}{3}\bigg)p^k_{m-1}(3) + \dots + \bigg(\frac{1}{k-1}\bigg)p^k_{m-1}(k-1) + \bigg(\frac{1}{k}\bigg)p^k_{m-1}(k) 
$$

$$
p^k_m(2) = 0  + \bigg(\frac{1}{2}\bigg) p^k_{m-1}(2) + \bigg(\frac{1}{3}\bigg)p^k_{m-1}(3) + \dots + \bigg(\frac{1}{k-1}\bigg)p^k_{m-1}(k-1) + \bigg(\frac{1}{k}\bigg)p^k_{m-1}(k) 
$$

$$
p^k_m(3) = 0  + 0 + \bigg(\frac{1}{3}\bigg)p^k_{m-1}(3) + \dots + \bigg(\frac{1}{k-1}\bigg)p^k_{m-1}(k-1) + \bigg(\frac{1}{k}\bigg)p^k_{m-1}(k) 
$$

$$
p^k_m(4) = 0 + 0 + 0  + \dots + \bigg(\frac{1}{k-1}\bigg)p^k_{m-1}(k-1) + \bigg(\frac{1}{k}\bigg)p^k_{m-1}(k) 
$$

$$
...
$$

$$
p^k_m(k) = 0 + 0 + 0  + \dots + 0 + \bigg(\frac{1}{k}\bigg)p^k_{m-1}(k) 
$$

I'm insisting on keeping the superscript $$k$$ because it's important to remember that these distributions are parametrized by $$k$$, i.e. the size of the initial sample space, as well as by $$m$$. We can think of the equations above as a matrix multiplication, making the following expression equivalent:

$$
    \begin{pmatrix}
      p^k_m(1) \\
      p^k_m(2) \\
      \dots \\
      p^k_m(k)
    \end{pmatrix}
    =
    \begin{pmatrix}
      1 & \frac{1}{2} & \frac{1}{3} & \dots & \frac{1}{k-1} & \frac{1}{k} \\
      0 & \frac{1}{2} & \frac{1}{3} & \dots  & \frac{1}{k-1} & \frac{1}{k} \\
      0 & 0 & \frac{1}{3} & \dots & \frac{1}{k-1} & \frac{1}{k} \\
      \vdots & \vdots & \vdots & \ddots & \frac{1}{k-1} & \frac{1}{k} \\
      0 & 0 & 0 & 0 & 0 & \frac{1}{k} 
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
      p^k_{m-1}(1) \\
      p^k_{m-1}(2) \\
      \dots \\
      p^k_{m-1}(k)\\
    \end{pmatrix}
$$

Rather than thinking of the probability distributions $$p^k_0, p^k_1, \dots p^k_m$$, then, we can think of a sequence of *probability vectors* $$\mathbf{P}^k_0, \mathbf{P}^k_1, \dots \mathbf{P}^k_m$$ satisfying

$$
\mathbf{P}^k_m = \mathbf{A}\mathbf{P}^k_{m-1}
$$

Where $$\mathbf{A}$$ is the transition matrix shown above. Then

$$\mathbf{P}^k_m = \mathbf{A}^m \mathbf{P}^k_0$$

Since we have that $$p^k_0(s) = \frac{1}{k}$$ for all $$s$$, $$\mathbf{P}^k_0 = \big(\frac{1}{k}, \frac{1}{k}, \dots \frac{1}{k} \big)$$. 

This expression, along with the base vector $$\mathbf{P}^k_0$$, defines a *matrix difference equation*, a well-studied topic (see for reference Ch. 7 in [Cull, Flahive and Robson: Difference Equations: From Rabbits to Chaos](https://link.springer.com/book/10.1007/0-387-27645-9)). 

## Pascal and Inverse Pascal

We take the standard approach to solving a matrix difference equation. Notice that $$\mathbf{A}$$ is an upper-triangular matrix, which means its eigenvalues can be read off the diagonal. These are $$1,\frac{1}{2}, \frac{1}{3}, \dots \frac{1}{k}$$. Because there are $$k$$ distinct eigenvalues, $$\mathbf{A}$$ [is diagonalizable](https://en.wikipedia.org/wiki/Diagonalizable_matrix). That means it can be written in the form

$$
\mathbf{A} = \mathbf{Q} \mathbf{D} \mathbf{Q}^{-1}
$$

Where $$\mathbf{D} = \text{diag}(1,\frac{1}{2}, \dots \frac{1}{k})$$ and the columns of $$Q$$ are the corresponding eigenvectors of $$\mathbf{A}$$. This is particularly useful in this case because we can write, for example:

$$
\mathbf{A}^2 = (\mathbf{Q}\mathbf{D}\mathbf{Q}^{-1} ) (\mathbf{Q}\mathbf{D}\mathbf{Q}^{-1}) = \mathbf{Q} \mathbf{D} (\mathbf{Q}^{-1}\mathbf{Q}) \mathbf{D} \mathbf{Q}^{-1} = \mathbf{Q}\mathbf{D}\mathbf{I}\mathbf{D}\mathbf{Q}^{-1} = \mathbf{Q}\mathbf{D}^2\mathbf{Q}^{-1}
$$

In general,

$$
\mathbf{A}^m = \mathbf{Q}\mathbf{D}^m\mathbf{Q}^{-1}
$$

With $$\mathbf{D}$$ being a diagonal matrix, $$\mathbf{D}^m$$ is easy to compute: $$\mathbf{D}^m = \text{diag}(1, \frac{1}{2^m}, \frac{1}{3^m}, \dots \frac{1}{k^m})$$

First we must find $$\mathbf{Q}$$ and $$\mathbf{Q}^{-1}$$:

#### Theorem

*The matrix $$\mathbf{Q}$$ whose columns are the eigenvectors of $$\mathbf{A}$$ is the **inverse Pascal matrix**, whose entries are given by:*

$$
Q_{ij} = \begin{cases} 
    (-1)^{j-i} \binom{j-1}{i-1}, & i \leq j \\
    0 & i > j \\
  \end{cases}
$$

*That is,*

$$
    \mathbf{Q}
    =
    \begin{pmatrix}
      1 & -1 & 1 & -1 & 1 & -1 & \dots \\
      0 &  1 &-2 &  3 &-4 &  5 & \dots \\
      0 & 0 &  1 & -3 & 6 &-10 & \dots \\
      0 & 0 & 0 &   1 &-4 & 10 & \dots \\
      0 & 0 & 0 &   0 & 1 & -5 & \dots \\
      \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots
    \end{pmatrix}
$$


#### Proof
<details>
  <summary>Click to expand proof</summary>
  

</details>

______________________________________________




#### Theorem

*$$\mathbf{Q}^{-1}$$ is equal to the **Pascal matrix**, whose entries are given by:*

$$
Q^{-1}_{ij} = \begin{cases} 
    \binom{j-1}{i-1}, & i \leq j \\
    0 & i > j \\
  \end{cases}
$$

*That is,*

$$
    \mathbf{Q}^{-1}
    =
    \begin{pmatrix}
      1 & 1 & 1 & 1 & 1 & 1 & \dots \\
      0 & 1 & 2 & 3 & 4 & 5 & \dots \\
      0 & 0 & 1 & 3 & 6 & 10 & \dots \\
      0 & 0 & 0 & 1 & 4 & 10 & \dots \\
      0 & 0 & 0 & 0 & 1 & 5 & \dots \\
      \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots
    \end{pmatrix}
$$

______________________________________________

With this information, we can begin to work on computing $$\mathbf{P}^k_m$$. First, we would like to find an expression for the entries of $$\mathbf{Q}\mathbf{D}^m\mathbf{Q}^{-1}$$. The first matrix product, $$\mathbf{D}^m\mathbf{Q}^{-1}$$, is:

$$
(D^mQ^{-1})_{ij} = \sum_{l = 1}^k (D^m)_{il} (Q^{-1})_{lj} = \sum_{l=1}^k \delta_{il} \bigg(\frac{1}{l}\bigg)^m \binom{j-1}{l-1} = \binom{j-1}{i-1}(\frac{1}{i})^m
$$

Then,

$$
(QD^m Q^{-1})_{ij} = \sum_{l=1}^k (-1)^{l-i} \binom{l-1}{i-1} \binom{j-1}{l-1} \bigg(\frac{1}{l}\bigg)^m
$$


<p markdown="1", style="background-color:grey">#### Theorem</p>

*The following identity holds*

$$  
\sum_{l=1}^k (-1)^{l-i} \binom{l-1}{i-1} \binom{j-1}{l-1} \bigg(\frac{1}{l}\bigg)^m = \binom{j-1}{i-1} \sum_{\gamma=0}^{j-i} (-1)^{\gamma} \binom{j-i}{\gamma} 
\bigg(\frac{1}{\gamma + i}\bigg)^m 
$$

*given that we discard terms in the sum containing $$\binom{n}{k}$$ for $$k>n$$ and $$k<0$$, which correspond to cases where $$i>j$$ in the matrices.*

#### Proof
<details><summary>Click to expand proof</summary>
  
    The following identity is true for the binomial coefficients:
    $$
    \binom{n}{m}\binom{m}{k} = \binom{n}{k}\binom{n-k}{m-k}
    $$
    Therefore, 
    $$
    \binom{j-1}{l-1}\binom{l-1}{i-1} = \binom{j-1}{i-1}\binom{j-1-(i-1)}{l-1-(i-1)} = \binom{j-1}{i-1}\binom{j-i}{l-i}
    $$
    Then the lefthand side of the equation is equal to
    $$
    \binom{j-1}{i-1} \sum_{l=1}^k (-1)^{l-i} \binom{j-i}{l-i} \bigg(\frac{1}{l}\bigg)^m
    $$
    We only allow terms where $$0 \leq l-i \leq j-i$$ so the limits of summation can be rewritten:
    $$
    \binom{j-1}{i-1} \sum_{l=i}^j (-1)^{l-i} \binom{j-i}{l-i} \bigg(\frac{1}{l}\bigg)^m
    $$
    Reindexing via $$\gamma=l-i$$ gives us the righthand side of the equation.
    </details>
