---
layout: post
title: "A 'wedding cake' probability distribution -- Part 2"
author: "Chris Langfield"
categories: math
tags: [math]
---

**You found the secret drafts area! Keep in mind the material below is in progress and subject to change prior to publishing.**

In [part 1](https://chris-langfield.github.io/wedding-cake) we explored the discrete probability distribution generated by a process of "double selection" from a set of integers. First we pick $$X_0 \sim \text{unif}(1\dots k)$$, and then we pick $$X_1 \sim \text{unif}(1\dots X_0)$$. The probability distribution of $$X_1$$ is

$$ Pr(X_1 = s) = \frac{1}{k} (H_k - H_{s-1}) $$

We proposed a sequence of probability distributions -- let's now call them $$p^k_m$$ -- corresponding to repeating this process $$m$$ times. That is, we pick an integer $$X_0$$ uniformly from $$1 \dots k$$, then we pick an integer $$X_1$$ uniformly from $$1 \dots X_0$$, pick $$X_2$$ from $$1 \dots X_1$$ and so on. Define 

$$p^k_0(s) = \frac{1}{k}$$

i.e. the base case of this process is simply the discrete uniform distribution. Then,

$$p^k_1(s) = \frac{1}{k} (H_k - H_{s-1})$$

This is what was derived in the last post. I mentioned that it is quite difficult to continue finding general expressions for $$p^k_m(s)$$ directly. However, there is a pattern with these distributions. First, recall:

$$Pr(X_m = s) = \sum_{i=1}^k Pr(X_m = s | X_{m-1} = i) Pr(X_{m-1} = i) $$

Using the same reasoning as in Part 1, we argue that 

$$
Pr(X_m=s|X_{m-1}=i) = \begin{cases} 
    \frac{1}{i} & s\leq i \\
    0 & s > i \\
  \end{cases}
$$

for every $$m$$. This is due to the fact that once $$X_{m-1}$$ has been chosen, $$X_m$$ is sampled *uniformly* from $$1\dots X_{m-1}$$

Therefore,

$$p^k_m(s) = \sum_{i=s}^k \frac{1}{i} p^k_{m-1}(i)$$

This recursive relationship can be expanded into $$k$$ equations:
$$
p^k_m(1) = 1 \cdot p_{m-1}(1) + \bigg(\frac{1}{2}\bigg)p^k_{m-1}(2) + \bigg(\frac{1}{3}\bigg)p^k_{m-1}(3) + \dots + \bigg(\frac{1}{k-1}\bigg)p^k_{m-1}(k-1) + \bigg(\frac{1}{k}\bigg)p^k_{m-1}(k) 
$$

$$
p^k_m(2) = 0  + \bigg(\frac{1}{2}\bigg) p^k_{m-1}(2) + \bigg(\frac{1}{3}\bigg)p^k_{m-1}(3) + \dots + \bigg(\frac{1}{k-1}\bigg)p^k_{m-1}(k-1) + \bigg(\frac{1}{k}\bigg)p^k_{m-1}(k) 
$$

$$
p^k_m(3) = 0  + 0 + \bigg(\frac{1}{3}\bigg)p^k_{m-1}(3) + \dots + \bigg(\frac{1}{k-1}\bigg)p^k_{m-1}(k-1) + \bigg(\frac{1}{k}\bigg)p^k_{m-1}(k) 
$$

$$
p^k_m(4) = 0 + 0 + 0  + \dots + \bigg(\frac{1}{k-1}\bigg)p^k_{m-1}(k-1) + \bigg(\frac{1}{k}\bigg)p^k_{m-1}(k) 
$$

$$
...
$$

$$
p^k_m(k) = 0 + 0 + 0  + \dots + 0 + \bigg(\frac{1}{k}\bigg)p^k_{m-1}(k) 
$$

I'm insisting on keeping the superscript $$k$$ because it's important to remember that these distributions are parametrized by $$k$$, i.e. the size of the initial sample space, as well as by $$m$$. We can think of the equations above as a matrix multiplication, making the following expression equivalent:

$$
    \begin{pmatrix}
      p^k_m(1) \\
      p^k_m(2) \\
      \dots \\
      p^k_m(k)
    \end{pmatrix}
    =
    \begin{pmatrix}
      1 & \frac{1}{2} & \frac{1}{3} & \dots & \frac{1}{k-1} & \frac{1}{k} \\
      0 & \frac{1}{2} & \frac{1}{3} & \dots  & \frac{1}{k-1} & \frac{1}{k} \\
      0 & 0 & \frac{1}{3} & \dots & \frac{1}{k-1} & \frac{1}{k} \\
      \vdots & \vdots & \vdots & \ddots & \frac{1}{k-1} & \frac{1}{k} \\
      0 & 0 & 0 & 0 & 0 & \frac{1}{k} 
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
      p^k_{m-1}(1) \\
      p^k_{m-1}(2) \\
      \dots \\
      p^k_{m-1}(k)\\
    \end{pmatrix}
$$

Rather than thinking of the probability distributions $$p^k_0, p^k_1, \dots p^k_m$$, then, we can think of a sequence of *probability vectors* $$\mathbf{P}^k_0, \mathbf{P}^k_1, \dots \mathbf{P}^k_m$$ satisfying

$$
\mathbf{P}^k_m = \mathbf{A}\mathbf{P}^k_{m-1}
$$

Where $$\mathbf{A}$$ is the transition matrix shown above. Then

$$\mathbf{P}^k_m = \mathbf{A}^m \mathbf{P}^k_0$$

Since we have that $$p^k_0(s) = \frac{1}{k}$$ for all $$s$$, $$\mathbf{P}^k_0 = \big(\frac{1}{k}, \frac{1}{k}, \dots \frac{1}{k} \big)$$. 

This expression, along with the base vector $$\mathbf{P}^k_0$$, defines a *matrix difference equation*, a well-studied topic (see for reference Ch. 7 in [Cull, Flahive and Robson: Difference Equations: From Rabbits to Chaos](https://link.springer.com/book/10.1007/0-387-27645-9)). 

I want to pause briefly here to recap. The problem we've set ourselves is to find an expression for the probability distribution over $$S = \{1, 2, \dots k\}$$ induced by repeating the selection process $$m$$ times. For $$m=0$$, this reduces to the uniform distribution over $$S$$, which we call $$p^k_0$$. It's possible to find an expression for $$p^k_1$$ in closed form. However, as $$m$$ grows, it becomes very difficult to simplify these equations. There is a recursive relationship between the distributions however. After noticing this, we rewrote the problem in terms of a recursive matrix equation. Our distributions $$p^k_0, p^k_1 \dots p^k_m$$ turned into probability vectors $$\mathbf{P}^k_0, \mathbf{P}^k_1, \dots, \mathbf{P}^k_m$$. 

The last step was to realize that we can write the $$m$$'th probability vector as the $$m$$'th power of the matrix $$\mathbf{A}$$ times the base vector $$\mathbf{P}^k_0$$. This is, finally, a closed form expression, although it is an unwieldy one. Our task now will be to decompress the matrix equation in the hopes of finding a function over $$S$$ describing the probability distribution for each $$m$$.

## Pascal and Inverse Pascal

We take the standard approach to solving a matrix difference equation. Notice that $$\mathbf{A}$$ is an upper-triangular matrix, which means its eigenvalues can be read off the diagonal. These are $$1,\frac{1}{2}, \frac{1}{3}, \dots \frac{1}{k}$$. Because there are $$k$$ distinct eigenvalues, $$\mathbf{A}$$ [is diagonalizable](https://en.wikipedia.org/wiki/Diagonalizable_matrix). That means it can be written in the form

$$
\mathbf{A} = \mathbf{Q} \mathbf{D} \mathbf{Q}^{-1}
$$

Where $$\mathbf{D} = \text{diag}(1,\frac{1}{2}, \dots \frac{1}{k})$$ and the columns of $$Q$$ are the corresponding eigenvectors of $$\mathbf{A}$$. This is particularly useful in this case because we can write, for example:

$$
\mathbf{A}^2 = (\mathbf{Q}\mathbf{D}\mathbf{Q}^{-1} ) (\mathbf{Q}\mathbf{D}\mathbf{Q}^{-1}) = \mathbf{Q} \mathbf{D} (\mathbf{Q}^{-1}\mathbf{Q}) \mathbf{D} \mathbf{Q}^{-1} = \mathbf{Q}\mathbf{D}\mathbf{I}\mathbf{D}\mathbf{Q}^{-1} = \mathbf{Q}\mathbf{D}^2\mathbf{Q}^{-1}
$$

In general,

$$
\mathbf{A}^m = \mathbf{Q}\mathbf{D}^m\mathbf{Q}^{-1}
$$

With $$\mathbf{D}$$ being a diagonal matrix, $$\mathbf{D}^m$$ is easy to compute. 

It wasn't difficult to find $$\mathbf{Q}$$ and $$\mathbf{Q}^{-1}$$:

#### Theorem

*The matrix $$\mathbf{Q}$$ whose columns are the eigenvectors of $$\mathbf{A}$$ is the **inverse Pascal matrix**, whose entries are given by:*

$$
Q_{ij} = \begin{cases} 
    (-1)^{j-i} \binom{j-1}{i-1}, & i \leq j \\
    0 & i > j \\
  \end{cases}
$$

*That is,*

$$
    \mathbf{Q}
    =
    \begin{pmatrix}
      1 & -1 & 1 & -1 & 1 & -1 & \dots \\
      0 &  1 &-2 &  3 &-4 &  5 & \dots \\
      0 & 0 &  1 & -3 & 6 &-10 & \dots \\
      0 & 0 & 0 &   1 &-4 & 10 & \dots \\
      0 & 0 & 0 &   0 & 1 & -5 & \dots \\
      \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots
    \end{pmatrix}
$$

______________________________________________




#### Theorem

*$$\mathbf{Q}^{-1}$$ is equal to the **Pascal matrix**, whose entries are given by:*

$$
Q^{-1}_{ij} = \begin{cases} 
    \binom{j-1}{i-1}, & i \leq j \\
    0 & i > j \\
  \end{cases}
$$

*That is,*

$$
    \mathbf{Q}^{-1}
    =
    \begin{pmatrix}
      1 & 1 & 1 & 1 & 1 & 1 & \dots \\
      0 & 1 & 2 & 3 & 4 & 5 & \dots \\
      0 & 0 & 1 & 3 & 6 & 10 & \dots \\
      0 & 0 & 0 & 1 & 4 & 10 & \dots \\
      0 & 0 & 0 & 0 & 1 & 5 & \dots \\
      \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots
    \end{pmatrix}
$$

______________________________________________

With this information, we can begin to work on computing $$\mathbf{P}^k_m$$. First, we would like to find an expression for the entries of $$\mathbf{Q}\mathbf{D}^m\mathbf{Q}^{-1}$$. The first matrix product, $$\mathbf{D}^m\mathbf{Q}^{-1}$$, is:

$$
(D^mQ^{-1})_{ij} = \sum_{l = 1}^k (D^m)_{il} (Q^{-1})_{lj} = \sum_{l=1}^k \delta_{il} (\frac{1}{l})^m \binom{j-1}{l-1} = \binom{j-1}{i-1}(\frac{1}{i})^m
$$

Then,

$$
(QD^m Q^{-1})_{ij} = \sum_{l=1}^k (-1)^{l-i} \binom{l-1}{i-1} \binom{j-1}{l-1} (frac{1}{l})^m
$$

#### Theorem

*The identity*

$$  
\sum_{l=1}^k (-1)^{l-i} \binom{l-1}{i-1} \binom{j-1}{l-1} (frac{1}{l})^m = \binom{j-1}{i-1} \sum_{\gamma=0}^{j-i} (-1)^{\gamma} \binom{j-i}{\gamma} (\frac{1}{\gamma})^m 
$$

*holds.*

#### Proof
<details>
  <summary>Click to expand proof</summary>
  
    The following identity is true for the binomial coefficients: (see 4.1.8 [this document](http://www.cs.columbia.edu/~cs4205/files/CM4.pdf), where two proofs are provided. From J. Gross, lecture notes for Combinatorial Mathematics.)
    $$
    \binom{n}{m}\binom{m}{k} = \binom{n}{k}\binom{n-k}{m-k}
    $$
    Therefore, 
    $$
    \binom{j-1}{l-1}\binom{l-1}{i-1} = \binom{j-1}{i-1}\binom{j-1-(i-1)}{l-1-(i-1)} = \binom{j-1}{i-1}\binom{j-i}{l-i}
    $$
</details>
