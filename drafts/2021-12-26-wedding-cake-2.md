---
layout: post
title: "A 'wedding cake' probability distribution -- Part 2"
author: "Chris Langfield"
categories: math
tags: [math]
---

**You found the secret drafts area! Keep in mind the material below is in progress and subject to change prior to publishing.**

In [part 1](https://chris-langfield.github.io/wedding-cake) we explored the discrete probability distribution generated by a process of "double selection" from a set of integers. First we pick $$X_0 \sim unif(1\dots k)$$, and then we pick $$X_1 \sim unif(1\dots X_0)$$. The probability distribution of $$X_1$$ is

$$ Pr(X_1 = s) = \frac{1}{k} (H_k - H_{s-1}) $$

We proposed a sequence of probability distributions -- let's now call them $$p^k_m$$ -- corresponding to repeating this process $$m$$ times. That is, we pick an integer $$X_0$$ uniformly from $$1 \dots k$$, then we pick an integer $$X_1$$ uniformly from $$1 \dots X_0$$, pick $$X_2$$ from $$1 \dots X_1$$ and so on. Define 

$$p^k_0(s) = \frac{1}{k}$$

i.e. the base case of this process is simply the discrete uniform distribution. Then,

$$p^k_1(s) = \frac{1}{k} (H_k - H_{s-1})$$

This is what was derived in the last post. I mentioned that it is quite difficult to continue finding general expressions for $$p^k_m(s)$$ directly. However, there is a pattern with these distributions. First, recall:

$$Pr(X_m = s) = \sum_{i=1}^k Pr(X_m = s | X_{m-1} = i) Pr(X_{m-1} = i) $$

Using the same reasoning as in Part 1, we argue that 

$$
Pr(X_m=s|X_{m-1}=i) = \begin{cases} 
    \frac{1}{i} & s\leq i \\
    0 & s > i \\
  \end{cases}
$$

for every $$m$$. This is due to the fact that once $$X_{m-1}$$ has been chosen, $$X_m$$ is sampled *uniformly* from $$1\dots X_{m-1}$$

Therefore,

$$p^k_m(s) = \sum_{i=s}^k \frac{1}{i} p^k_{m-1}(i)$$

This recursive relationship can be expanded into $$k$$ equations:
$$
p^k_m(1) = 1 \cdot p_{m-1}(1) + \bigg(\frac{1}{2}\bigg)p^k_{m-1}(2) + \bigg(\frac{1}{3}\bigg)p^k_{m-1}(3) + \dots + \bigg(\frac{1}{k-1}\bigg)p^k_{m-1}(k-1) + \bigg(\frac{1}{k}\bigg)p^k_{m-1}(k) 
$$

$$
p^k_m(2) = 0  + \bigg(\frac{1}{2}\bigg) p^k_{m-1}(2) + \bigg(\frac{1}{3}\bigg)p^k_{m-1}(3) + \dots + \bigg(\frac{1}{k-1}\bigg)p^k_{m-1}(k-1) + \bigg(\frac{1}{k}\bigg)p^k_{m-1}(k) 
$$

$$
p^k_m(3) = 0  + 0 + \bigg(\frac{1}{3}\bigg)p^k_{m-1}(3) + \dots + \bigg(\frac{1}{k-1}\bigg)p^k_{m-1}(k-1) + \bigg(\frac{1}{k}\bigg)p^k_{m-1}(k) 
$$

$$
p^k_m(4) = 0 + 0 + 0  + \dots + \bigg(\frac{1}{k-1}\bigg)p^k_{m-1}(k-1) + \bigg(\frac{1}{k}\bigg)p^k_{m-1}(k) 
$$

$$
...
$$

$$
p^k_m(k) = 0 + 0 + 0  + \dots + 0 + \bigg(\frac{1}{k}\bigg)p^k_{m-1}(k) 
$$

or equivalently,

$$
    \begin{pmatrix}
      p^k_m(1) \\
      p^k_m(2) \\
      \dots \\
      p^k_m(k)
    \end{pmatrix}
    =
    \begin{pmatrix}
      1 & \frac{1}{2} & \frac{1}{3} & \dots & \frac{1}{k-1} & \frac{1}{k} \\
      0 & \frac{1}{2} & \frac{1}{3} & \dots  & \frac{1}{k-1} & \frac{1}{k} \\
      0 & 0 & \frac{1}{3} & \dots & \frac{1}{k-1} & \frac{1}{k} \\
      \vdots & \vdots & \vdots & \ddots & \frac{1}{k-1} & \frac{1}{k} \\
      0 & 0 & 0 & 0 & 0 & \frac{1}{k} 
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
      p^k_{m-1}(1) \\
      p^k_{m-1}(2) \\
      \dots \\
      p^k_{m-1}(k)\\
    \end{pmatrix}
$$

Rather than thinking of the probability distributions $$p^k_0, p^k_1, \dots p^k_m$$, then, we can think of a sequence of *probability vectors* $$\mathbf{P}^k_0, \mathbf{P}^k_1, \dots \mathbf{P}^k_m$$ satisfying

$$
\mathbf{P}^k_m = \mathbf{A}\mathbf{P}^k_{m-1}
$$

Where $$\mathbf{A}$$ is the transition matrix shown above. Then

$$\mathbf{P}^k_m = \mathbf{A}^m \mathbf{P}^k_0$$

Since we have that $$p^k_0(s) = \frac{1}{k}$$ for all $$s$$, $$\mathbf{P}^k_0 = \big(\frac{1}{k}, \frac{1}{k}, \dots \frac{1}{k} \big)$$. 

This expression, along with the base vector $$\mathbf{P}^k_0$$, defines a *matrix difference equation*, a well-studied topic (see for reference Ch. 7 in [Cull, Flahive and Robson: Difference Equations: From Rabbits to Chaos](https://link.springer.com/book/10.1007/0-387-27645-9)). 

I want to pause briefly here to recap. The problem we've set ourselves is to find an expression for the probability distribution over $$S = \{1, 2, \dots k\}$$ induced by repeating the selection process $$m$$ times. For $$m=0$$, this reduces to the uniform distribution over $$S$$, which we call $$p^k_0$$. It's possible to find an expression for $$p^k_1$$ in closed form. However, as $$m$$ grows, it becomes very difficult to simplify these equations. There is a recursive relationship between the distributions however. After noticing this, we rewrote the problem in terms of a recursive matrix equation. Our distributions $$p^k_0, p^k_1 \dots$$ turned into probability vectors $$\mathbf{P}^k_0, \mathbf{P}^k_1, \dots$$. The last step was to realize that we can write the $$m$$'th probability vector as the $$m$$'th power of the matrix $$\mathbf{A}$$ times the base vector $$\mathbf{P}^k_0$$. This is, finally, a closed form expression, although it is an unwieldy one. Our task now will be to decompress the matrix equation in the hopes of finding a function over $$S$$ describing the probability distribution for each $$m$$.
